{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L_9qG88nVx_9",
        "outputId": "679e26e6-7995-4f9d-e70b-b7011bfd1d83"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import backend as K\n",
        "from google.colab import drive\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.layers import GRU, Dense, Bidirectional,LSTM\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "import keras\n",
        "\n",
        "def Normalize(X):\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  X_reshaped = X.reshape(-1, X.shape[-1])\n",
        "  scaler = StandardScaler()\n",
        "  X_scaled = scaler.fit_transform(X_reshaped)\n",
        "  X_scaled = X_scaled.reshape(X.shape)\n",
        "  return X_scaled\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "pickle_path = '/content/drive/MyDrive/pickle'\n",
        "txt_path = os.path.join(pickle_path, 'kelimeler.txt')\n",
        "\n",
        "with open(txt_path, 'r', encoding='utf-8') as f:\n",
        "    kelime_listesi = [line.strip() for line in f if line.strip()]\n",
        "numclass = len(kelime_listesi)\n",
        "\n",
        "X = pickle.load(open(os.path.join(pickle_path, \"X446Y_hands.pickle\"), \"rb\"))\n",
        "Y = pickle.load(open(os.path.join(pickle_path, \"Y446Y_hands.pickle\"), \"rb\"))\n",
        "print(X.shape)\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.125, stratify=Y_train, random_state=42)\n",
        "\n",
        "X_train = Normalize(X_train)\n",
        "X_val = Normalize(X_val)\n",
        "X_test = Normalize(X_test)\n",
        "\n",
        "# One-hot encoding\n",
        "Y_train = to_categorical(Y_train, num_classes=numclass)\n",
        "Y_val = to_categorical(Y_val, num_classes=numclass)\n",
        "Y_test = to_categorical(Y_test, num_classes=numclass)\n",
        "\n",
        "#GRU+Bi-LSTM Model\n",
        "model = Sequential()\n",
        "dropvalue=0.3\n",
        "model.add(GRU(512, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(Dropout(dropvalue))  \n",
        "model.add(GRU(256, return_sequences=True))\n",
        "model.add(Dropout(dropvalue))  \n",
        "model.add(Bidirectional(LSTM(128)))   \n",
        "model.add(BatchNormalization()) \n",
        "model.add(Dropout(dropvalue))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(numclass, activation='softmax'))\n",
        "\n",
        "# --- Select Optimizer ---\n",
        "k = 0\n",
        "if (k == 0): opt = optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.9)\n",
        "if (k == 1): opt = keras.optimizers.Adadelta(learning_rate=0.01)\n",
        "if (k == 2): opt = optimizers.SGD(learning_rate=0.001, clipnorm=1.)\n",
        "if (k == 3): opt = keras.optimizers.RMSprop(learning_rate=0.0001, rho=0.9, decay=0.0)\n",
        "if (k == 4): opt = keras.optimizers.Adamax(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, decay=0.0)\n",
        "if (k == 5): opt = keras.optimizers.Nadam(learning_rate=0.0004, beta_1=0.9, beta_2=0.999)\n",
        "if (k == 6): opt = optimizers.AdamW(learning_rate=0.0001, beta_1=0.9, beta_2=0.9)\n",
        "\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# --- CALLBACKS ---\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=15, min_lr=1e-6, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint(filepath='/content/drive/MyDrive/pickle/GRU-BÄ°LSTM_best_model.keras',\n",
        "                                   monitor='val_loss', save_best_only=True, verbose=1)\n",
        "# --- Train Time trace ---\n",
        "start_time = time.time()\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, Y_train,\n",
        "    validation_data=(X_val, Y_val),\n",
        "    epochs=1000,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stopping, reduce_lr, model_checkpoint],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "training_duration = end_time - start_time\n",
        "\n",
        "# --- Test Evaluation ---\n",
        "test_loss, test_accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(f\"\\n Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"\\n Test Loss: {test_loss:.4f}\")\n",
        "print(f\"\\n Train Time: {training_duration:.2f} saniye\")\n",
        "print(f\"\\n Total Parameters: {model.count_params()}\")\n",
        "\n",
        "Y_pred = model.predict(X_test)\n",
        "Y_pred_labels = np.argmax(Y_pred, axis=1)\n",
        "Y_true_labels = np.argmax(Y_test, axis=1)\n",
        "\n",
        "f1_macro = f1_score(Y_true_labels, Y_pred_labels, average='macro')\n",
        "print(f\"F1-Macro Skoru: {f1_macro:.4f}\")\n",
        "\n",
        "result = classification_report(Y_true_labels, Y_pred_labels, digits=4, output_dict=False)\n",
        "print(result)\n",
        "\n",
        "# Accuracy and Loss Curves\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='train loss')\n",
        "plt.plot(history.history['val_loss'], label='validation loss')\n",
        "plt.title('Loss Curve')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='train accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='validation accuracy')\n",
        "plt.title('Accuracy Curve')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
